{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"name":"News Headline Generation.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g-thOm8Buzp9","executionInfo":{"status":"ok","timestamp":1605860947395,"user_tz":-330,"elapsed":5720,"user":{"displayName":"Ishita Batra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNAlVHB7P4kGCQE8S5QInNpTMy355QtlQxCiRG=s64","userId":"12074113989494675974"}},"outputId":"bd8d18b9-d578-4c9a-e2f6-fa9de4a3db09"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","\n","from gensim.models import word2vec, FastText\n","\n","from keras.models import Sequential, Model\n","from keras.layers import LSTM, Dense, Dropout, Embedding, Bidirectional\n","from keras.callbacks import ModelCheckpoint\n","from keras.layers.merge import add, concatenate\n","from keras import Input\n","from keras.utils import to_categorical\n","import tensorflow as tf\n","!pip install \"nltk==3.4.5\"\n","from nltk.translate.bleu_score import sentence_bleu\n","from nltk.translate.meteor_score import meteor_score"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.6/dist-packages (3.4.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.5) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m4Dn9iVguzp_"},"source":["np.random.seed(105)\n","tf.random.set_seed(105)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"liaCoQ0xuzp_"},"source":["## Import Data "]},{"cell_type":"code","metadata":{"id":"24LHYPCPuzp_"},"source":["df = pd.read_csv('/content/keyworded.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"rGkcPsiYuzp_","executionInfo":{"status":"ok","timestamp":1605861551161,"user_tz":-330,"elapsed":1137,"user":{"displayName":"Ishita Batra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNAlVHB7P4kGCQE8S5QInNpTMy355QtlQxCiRG=s64","userId":"12074113989494675974"}},"outputId":"c5fd4a58-6b0c-4c00-a166-b2b324680e1c"},"source":["df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Unnamed: 0.1</th>\n","      <th>heads</th>\n","      <th>descs</th>\n","      <th>keywords</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>inclement weather prevents liar from getting t...</td>\n","      <td>PROVIDENCE RI—In spite of his best efforts to ...</td>\n","      <td>{'carlson': 0.25, 'weather': 0.196, 'spotty': ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>mother comes pretty close to using word stream...</td>\n","      <td>PATERSON NJ—Family sources told reporters Tues...</td>\n","      <td>{'burkhart': 0.356, 'she': 0.283, 'close': 0.2...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>richard bransons globalwarming donation nearly...</td>\n","      <td>LONDON—Analysts are predicting that the 3 bill...</td>\n","      <td>{'branson': 0.439, 'virgin': 0.235, 'balloonba...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>shadow government getting too large to meet in...</td>\n","      <td>COLUMBUS OH—With its membership swelling in re...</td>\n","      <td>{'marriotts': 0.177, 'meeting': 0.176, 'confer...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>ford develops new suv that runs purely on gaso...</td>\n","      <td>DEARBORN MI—The Ford Motor Company announced W...</td>\n","      <td>{'gasoline': 0.354, 'petrola': 0.296, 'nair': ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  ...                                           keywords\n","0           0  ...  {'carlson': 0.25, 'weather': 0.196, 'spotty': ...\n","1           1  ...  {'burkhart': 0.356, 'she': 0.283, 'close': 0.2...\n","2           2  ...  {'branson': 0.439, 'virgin': 0.235, 'balloonba...\n","3           3  ...  {'marriotts': 0.177, 'meeting': 0.176, 'confer...\n","4           4  ...  {'gasoline': 0.354, 'petrola': 0.296, 'nair': ...\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AX1Tv2IwuzqA","executionInfo":{"status":"ok","timestamp":1605861116859,"user_tz":-330,"elapsed":1032,"user":{"displayName":"Ishita Batra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNAlVHB7P4kGCQE8S5QInNpTMy355QtlQxCiRG=s64","userId":"12074113989494675974"}},"outputId":"bc3ddd69-77fc-4d41-8530-a699a0126ab0"},"source":["df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10580, 3)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"K5ebuQbXuzqB"},"source":["# use the lemma_tokens as model input\n","X_body = [eval(x) for x in df['lemma_tokens'].values]\n","X_is_sarcastic = df.is_sarcastic.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cPFbGR25uzqB"},"source":["headlines = [eval(x) for x in df['tokens.1'].values]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EVJdWIjouzqB"},"source":["## Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"HUTvuRYsuzqB"},"source":["### Convert Entities Data"]},{"cell_type":"code","metadata":{"id":"RpImDWDTuzqB"},"source":["entities = [eval(x) for x in df['entities'].values]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XqBIJ1zluzqB"},"source":["# remove type of the entities\n","temp_entities = []\n","\n","for row_entities in entities:\n","    temp = []\n","    for typed_entities in row_entities:\n","        temp += typed_entities[1]\n","    temp_entities.append(list(set(temp)))\n","\n","entities = temp_entities"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aVFokgRduzqB"},"source":["# lowercase all entities\n","for row_entities in entities:\n","    for i in range(len(row_entities)):\n","        row_entities[i] = row_entities[i].lower()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tZkfQkyuuzqB"},"source":["X_is_entity = []\n","\n","for i, (words, row_entities) in enumerate(zip(X_body, entities)):\n","    X_is_entity.append([])\n","    for word in words:\n","        if word in row_entities:\n","            X_is_entity[i].append(1)\n","        else:\n","            X_is_entity[i].append(0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MoFLlShQuzqB"},"source":["### Convert Keywords Data"]},{"cell_type":"code","metadata":{"id":"pheIUACKuzqB"},"source":["keywords = [eval(x) for x in df['keywords'].values]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X7E257aLuzqB"},"source":["keywords = [list(keyword.keys()) for keyword in keywords]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E3g-OiIDuzqB"},"source":["X_is_keyword = []\n","\n","for i, (words, row_keywords) in enumerate(zip(X_body, keywords)):\n","    X_is_keyword.append([])\n","    for word in words:\n","        if word in row_keywords:\n","            X_is_keyword[i].append(1)\n","        else:\n","            X_is_keyword[i].append(0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AtHfkdxSuzqC"},"source":["### Word Embedding"]},{"cell_type":"code","metadata":{"id":"fBsXy2v-uzqC"},"source":["def embed_words(embedding_model, sentence):\n","    vector = []\n","    for i in range(len(sentence)):\n","        vector.append(embedding_model[sentence[i]])\n","    \n","    return vector"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ytIWdDWeuzqC"},"source":["#### Word2Vec"]},{"cell_type":"code","metadata":{"id":"NNvkH4R5uzqC"},"source":["w2v = word2vec.Word2Vec\n","\n","body_embedding = w2v.load('models/body_embedding.model')\n","head_embedding = w2v.load('models/head_embedding.model')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j0nys_D9uzqC"},"source":["#### FastText"]},{"cell_type":"code","metadata":{"id":"U98VcIE1uzqC"},"source":["body_embedding = FastText.load('models/body_embedding_fasttext.model')\n","head_embedding = FastText.load('models/head_embedding_fasttext.model')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d4cvQ45GuzqC"},"source":["### Padding"]},{"cell_type":"markdown","metadata":{"id":"or5iFO8GuzqC"},"source":["#### Pad Body"]},{"cell_type":"code","metadata":{"id":"PLyIoihKuzqC"},"source":["BODY_LENGTH = max([len(x_body) for x_body in X_body])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-UHat2GYuzqC"},"source":["EMBEDDING_DIM = 103"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bUgs3ai4uzqC"},"source":["def pad_body(body):\n","    return [np.zeros(EMBEDDING_DIM)] * (BODY_LENGTH - len(body)) + body"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pAdNQVReuzqC"},"source":["#### Create Function for Pad Head"]},{"cell_type":"code","metadata":{"id":"3x1C9DruuzqD"},"source":["HEAD_LENGTH = max([len(headline) for headline in headlines])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7X_ky88JuzqD"},"source":["HEAD_EMBEDDING_DIM = 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uBUgyTTyuzqD"},"source":["def pad_head(headline):\n","    return [np.zeros(HEAD_EMBEDDING_DIM)] * (HEAD_LENGTH - len(headline)) + headline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oG2Kt4o_uzqD"},"source":["### Create Dictionary for Head Word Index"]},{"cell_type":"code","metadata":{"id":"dqKZ5AxzuzqD"},"source":["words_list = list(set([inner for outer in headlines for inner in outer]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L6T2tNozuzqD"},"source":["idx_to_word = {i: word for i, word in enumerate(words_list)}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"donY7WDVuzqD"},"source":["word_to_idx = {word: i for i, word in enumerate(words_list)}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lF4Kq4PiuzqD"},"source":["### Count Head Vocab Size"]},{"cell_type":"code","metadata":{"id":"i1SEySFkuzqD"},"source":["HEAD_VOCAB_SIZE = len(words_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2IxQCKJfuzqD"},"source":["## Create Model"]},{"cell_type":"markdown","metadata":{"id":"YK7S8TQiuzqE"},"source":["### Data Generator"]},{"cell_type":"code","metadata":{"id":"XgJO_KqfuzqE"},"source":["# data generator, intended to be used in a call to model.fit_generator()\n","def data_generator(X_body, X_is_entity, X_is_keyword, X_is_sarcastic, headlines, batch_size):\n","    while 1:\n","        for x_body, x_is_entity, x_is_keyword, x_is_sarcastic, headline in zip(X_body, X_is_entity, X_is_keyword, X_is_sarcastic, headlines):\n","            # word embedding\n","            x_body_embedded = embed_words(body_embedding, x_body)\n","            headline_embedded = embed_words(head_embedding, headline)\n","            \n","            # concat x_body\n","            for i in range(len(x_body_embedded)):\n","                x_body_embedded[i] = np.concatenate(([x_is_entity[i]], [x_is_keyword[i]], \n","                                                     [x_is_sarcastic], x_body_embedded[i]))\n","            # pad x_body\n","            x_body_embedded = pad_body(x_body_embedded)\n","            # reshape\n","            x_body_embedded = np.array(x_body_embedded).reshape(1, BODY_LENGTH, EMBEDDING_DIM)\n","            \n","            for i in range(1, len(headline)):\n","                # split into input and output pair\n","                in_seq, out_seq = headline_embedded[:i], headline[i]\n","                # pad input sequence\n","                in_seq = pad_head(in_seq)\n","                # reshape\n","                in_seq = np.array(in_seq).reshape(1, HEAD_LENGTH, HEAD_EMBEDDING_DIM)\n","        \n","                yield [np.array(x_body_embedded), np.array(in_seq)], np.array([to_categorical(word_to_idx[out_seq], num_classes=HEAD_VOCAB_SIZE)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S7eshRfIHkoJ"},"source":["def generate_headlines(model_name, starting_words, input_length, rnn_size):\n","  \n","    tf.reset_default_graph() \n","\n","    with tf.Session() as sess:\n","\n","        model_path = 'model/'\n","        model = Model(input_length=input_length, rnn_size=rnn_size, vocab_size=vocab_size)\n","\n","        try:\n","            model.saver.restore(sess, model_path+model_name)\n","            print(f'Model {model_name} Restored')\n","            \n","        except:\n","            print(f'Model {f} does not exist')\n","            return None\n","          \n","        generated_sentences = []\n","        \n","\n","        \n","        for starting_word in starting_words:\n","\n","            generated = [start] + numerize(starting_word)\n","\n","            while len(generated) < input_length:\n","\n","                # Pad current generated sentence to match the input_length\n","                padded = generated[:input_length] + [pad] * (input_length - len(generated))\n","                padded = np.array([padded])\n","\n","                feed = {model.input_num : padded}\n","\n","                logits = sess.run(model.logits, feed_dict=feed)\n","\n","                last_logits = logits[0][len(generated)-1][5:]\n","\n","                generated.append(np.argmax(last_logits)+5)\n","\n","\n","            generated_sentence = translate_numerized(generated)\n","\n","            generated_sentences.append(generated_sentence)\n","            \n","    return generated_sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"isTHqtx5HpIo"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ELf93zx-HpuV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b6-iEZPDuzqE"},"source":["### Model Architecture"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"BfmrSVL8uzqE"},"source":["input_body = Input(shape=(BODY_LENGTH, EMBEDDING_DIM), name='input_body')\n","do_body = Dropout(0)(input_body)\n","lstm_body = LSTM(64)(do_body)\n","\n","input_head = Input(shape=(HEAD_LENGTH, HEAD_EMBEDDING_DIM), name='input_head')\n","do_head = Dropout(0)(input_head)\n","lstm_head = LSTM(64)(do_head)\n","\n","decoder1 = add([lstm_body, lstm_head])\n","decoder2 = Dense(1024, activation='relu', name='dense_decoder')(decoder1)\n","output = Dense(HEAD_VOCAB_SIZE, activation='softmax', name='output')(decoder2)\n","\n","model = Model(inputs=[input_body, input_head], outputs=output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q-lFqgOYuzqE","outputId":"418ce296-03c4-4a46-edbb-1380a86ca681"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model_4\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_body (InputLayer)         (None, 6488, 103)    0                                            \n","__________________________________________________________________________________________________\n","input_head (InputLayer)         (None, 31, 100)      0                                            \n","__________________________________________________________________________________________________\n","dropout_7 (Dropout)             (None, 6488, 103)    0           input_body[0][0]                 \n","__________________________________________________________________________________________________\n","dropout_8 (Dropout)             (None, 31, 100)      0           input_head[0][0]                 \n","__________________________________________________________________________________________________\n","lstm_7 (LSTM)                   (None, 64)           43008       dropout_7[0][0]                  \n","__________________________________________________________________________________________________\n","lstm_8 (LSTM)                   (None, 64)           42240       dropout_8[0][0]                  \n","__________________________________________________________________________________________________\n","add_4 (Add)                     (None, 64)           0           lstm_7[0][0]                     \n","                                                                 lstm_8[0][0]                     \n","__________________________________________________________________________________________________\n","dense_decoder (Dense)           (None, 1024)         66560       add_4[0][0]                      \n","__________________________________________________________________________________________________\n","output (Dense)                  (None, 5409)         5544225     dense_decoder[0][0]              \n","==================================================================================================\n","Total params: 5,696,033\n","Trainable params: 5,696,033\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dBgQbCzRuzqE"},"source":["model.compile(loss='categorical_crossentropy', optimizer='adam')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-OxCkda5uzqE"},"source":["# load model\n","# model.load_weights('./models/model_test19.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dRSTj7h2uzqE"},"source":["### Training"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"IOIl7DBguzqE","outputId":"efe77968-544f-4842-aa11-30cd2ec2d4b2"},"source":["epochs = 15\n","batch_size = 32\n","\n","steps = len(X_body) // batch_size\n","\n","for i in range(6, epochs):\n","    generator = data_generator(X_body, X_is_entity, X_is_keyword, X_is_sarcastic, headlines, batch_size)\n","    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n","    model.save('./models/model_w2v_e' + str(i) + '.h5')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/1\n"],"name":"stdout"},{"output_type":"stream","text":["D:\\Programs\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  after removing the cwd from sys.path.\n"],"name":"stderr"},{"output_type":"stream","text":["312/312 [==============================] - 1466s 5s/step - loss: 1.8235\n","Epoch 1/1\n","312/312 [==============================] - 1469s 5s/step - loss: 1.5804\n","Epoch 1/1\n","312/312 [==============================] - 1462s 5s/step - loss: 1.2342\n","Epoch 1/1\n","312/312 [==============================] - 1461s 5s/step - loss: 1.1348\n","Epoch 1/1\n","312/312 [==============================] - 1462s 5s/step - loss: 0.9382\n","Epoch 1/1\n","312/312 [==============================] - 1463s 5s/step - loss: 0.9579\n","Epoch 1/1\n","312/312 [==============================] - 1464s 5s/step - loss: 0.9536\n","Epoch 1/1\n","312/312 [==============================] - 1474s 5s/step - loss: 0.6922\n","Epoch 1/1\n","312/312 [==============================] - 1465s 5s/step - loss: 0.5637\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"39kdL6DGuzqF"},"source":["### Predicting"]},{"cell_type":"code","metadata":{"id":"-vAKHWdPuzqF"},"source":["def predict(x_body, x_is_entity, x_is_keyword, x_is_sarcastic):\n","    # word embedding\n","    x_body_embedded = embed_words(body_embedding, x_body)\n","\n","    # concat x_body\n","    for i in range(len(x_body_embedded)):\n","        x_body_embedded[i] = np.concatenate(([x_is_entity[i]], [x_is_keyword[i]], \n","                                             [x_is_sarcastic], x_body_embedded[i]))\n","\n","    # pad x_body\n","    x_body_embedded = pad_body(x_body_embedded)\n","    x_body_embedded = np.array(x_body_embedded).reshape(1, BODY_LENGTH, EMBEDDING_DIM)\n","    in_text = '<startseq>'\n","    for i in range(HEAD_LENGTH):\n","        sequence = embed_words(head_embedding, in_text.split())\n","        sequence = pad_head(sequence)\n","        sequence = np.array(sequence).reshape(1, HEAD_LENGTH, HEAD_EMBEDDING_DIM)\n","        yhat = model.predict([x_body_embedded, sequence], verbose=0)\n","        \n","        yhat = np.argmax(yhat)\n","        word = idx_to_word[yhat]\n","#         print(word)\n","        in_text += ' ' + word\n","        if word == '<endseq>':\n","            break\n","    \n","    in_text = in_text.replace('<startseq>','')\n","    in_text = in_text.replace('<endseq>','')\n","\n","    return in_text.strip().split()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"8C2ar4vMuzqF","outputId":"51f83d15-faab-4c57-a663-57c95c88c317"},"source":["i = 5\n","y_pred = predict(X_body[i], X_is_entity[i], X_is_keyword[i], X_is_sarcastic[i])\n","y_pred"],"execution_count":null,"outputs":[{"output_type":"stream","text":["D:\\Programs\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  after removing the cwd from sys.path.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["['its',\n"," 'you',\n"," 'you',\n"," 'you',\n"," 'me',\n"," 'are',\n"," 'your',\n"," 'back',\n"," 'are',\n"," 'back',\n"," 'back',\n"," 'back',\n"," 'back']"]},"metadata":{"tags":[]},"execution_count":94}]},{"cell_type":"code","metadata":{"id":"sDCBiYaXuzqF","outputId":"470109ba-28fa-4072-dbe2-a46ab2f45841"},"source":["headlines[i][1:-1]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['its', 'not', 'you', 'its', 'me', 'are', 'your', 'holding', 'you', 'back']"]},"metadata":{"tags":[]},"execution_count":95}]},{"cell_type":"code","metadata":{"id":"YLlDSBJtIJ04"},"source":["starting_words = ['court', 'samsung', 'apple', 'google', 'google and apple', 'google and samsung', 'samsung and apple']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tQb3q3xXuzqF","outputId":"0b4281c4-1c2f-40b1-9fe1-188d2b4a923c"},"source":["bleu_score(headlines[i][1:-1], y_pred)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1.0, 1.0, 1.0, 1.0]"]},"metadata":{"tags":[]},"execution_count":102}]},{"cell_type":"markdown","metadata":{"id":"t9d5BwlJuzqF"},"source":["### Testing"]},{"cell_type":"markdown","metadata":{"id":"BPbP_IUYuzqF"},"source":["##### BLEU Score"]},{"cell_type":"code","metadata":{"id":"CCd2tEWYuzqF"},"source":["def bleu_score(reference, generated):\n","    bleu1 = sentence_bleu([reference], generated, weights=(1, 0, 0, 0))\n","    bleu2 = sentence_bleu([reference], generated, weights=(0.5, 0.5, 0, 0))\n","    bleu3 = sentence_bleu([reference], generated, weights=(0.33, 0.33, 0.33, 0))\n","    bleu4 = sentence_bleu([reference], generated, weights=(0.25, 0.25, 0.25, 0.25))\n","    \n","    return [bleu1, bleu2, bleu3, bleu4]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JafQYrWQuzqF","outputId":"a087ec07-5ef4-4ac0-f59d-a5f9b02992f9"},"source":["bleu_scores = []\n","for i in range(100):\n","    y_pred = predict(X_body[i], X_is_entity[i], X_is_keyword[i], X_is_sarcastic[i])\n","    bleu_scores.append(bleu_score(headlines[i][1:-1], y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["D:\\Programs\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  after removing the cwd from sys.path.\n","D:\\Programs\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n","The hypothesis contains 0 counts of 2-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","D:\\Programs\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n","The hypothesis contains 0 counts of 3-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","D:\\Programs\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"i8LadObAuzqG"},"source":["bleu_scores_avg = np.array(bleu_scores).mean(axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"YgkpObEluzqG","outputId":"0c07a05f-ebdd-4cd1-a0bb-4a161b9d1aa9"},"source":["print('BLEU-1: %.3f' % bleu_scores_avg[0])\n","print('BLEU-2: %.3f' % bleu_scores_avg[1])\n","print('BLEU-3: %.3f' % bleu_scores_avg[2])\n","print('BLEU-4: %.3f' % bleu_scores_avg[3])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["BLEU-1: 0.259\n","BLEU-2: 0.225\n","BLEU-3: 0.194\n","BLEU-4: 0.144\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xXW8VlxHuzqG"},"source":["##### METEOR Score"]},{"cell_type":"code","metadata":{"id":"a0aBoPj1uzqG","outputId":"d310824d-3fd7-4d67-9849-0df95d237d24"},"source":["meteor_scores = []\n","for i in range(100):\n","    y_pred = predict(X_body[i], X_is_entity[i], X_is_keyword[i], X_is_sarcastic[i])\n","    meteor_scores.append(meteor_score([' '.join(headlines[i][1:-1])], ' '.join(y_pred)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["D:\\Programs\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  after removing the cwd from sys.path.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"2AKODIhZuzqG"},"source":["meteor_scores_avg = np.array(meteor_scores).mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kx2CUMacuzqG","outputId":"f01b66c8-56ab-48f1-8d83-3fd69a008434"},"source":["print('METEOR: %.3f' % meteor_scores_avg)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["METEOR: 0.270\n"],"name":"stdout"}]}]}